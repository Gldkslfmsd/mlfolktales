ÄŒt dub 27 15:28:33 CEST 2017 [Dominik]

Classification
==============

Data description and preprocessing:
-- sie plots in plots/ dir
-- let's start only with English, there are enough labeled stories (342 in
total)
-- let's start with atu_level_1 due to lack of data
-- we split data by every category:
	-- 70 % of training
	-- 30 % of testing

	252:90
	
	
Feature selection:
-- most frequent words in each category
    -- remove diacritics and stopwords because they are the most frequent in all texts and we can't distinguish them by it
    -- take N mfw for each category
        -- remove the words which are in more than 1 groups
        -- estimate N by hand so every category has several of such words
            -- we ended with 20
    -- make features from it:
        #1 number of occurences in a text, not normalized
        (-- or number of occurences per word in a text)


Classification:
-- several common classifiers from sklearn were used
-- baseline is 0.51 (returning always the most frequent class)

#1 0.71 ("Linear SVM",SVC(kernel="linear", C=0.025))