ÄŒt dub 27 15:28:33 CEST 2017 [Dominik]

Classification
==============

Data description and preprocessing:
-- sie plots in plots/ dir
-- let's start only with English, there are enough labeled stories (342 in
total)
-- let's start with atu_level_1 due to lack of data
-- we split data by every category:
	-- 70 % of training
	-- 30 % of testing

	252:90
	
	
Feature selection:
-- remove diacritics and stopwords because they are the most frequent in all texts and we can't distinguish them by it
-- we compared different strategies 


        
#1.0 simple most frequent words
-- take N mfw for each category
  -- estimate N by hand so every category has several of such words
  -- we ended with 20
-- values are numbers of occurences in a text
-- not scaled
-- we had 42 such words, see keywords_for_features.py
-- see plots/keyword_features_version1.pdf

#1.1
-- same as #1, but 
    -- MinMaxScaling were used (it's better with it)
    -- not total number of occurences in a text, but number of occ/text length, 
    
#2.0
-- take N words by highest value of col_freq[w]**doc_freq[w]
    -- col_freq is a total number of occurences in all text in a class
    -- doc_freq is a number of documents in a class containing given word
    -- N = 100
-- don't remove words chosen from 2 classes
-- in total, 242 words
-- scaled
-- see plots/weighted_keyword_features.pdf

#2.1
-- order features by forest features importance
    -- see plots/forest_feature_importance.pdf
-- use greedy forward selection to find optimal number of features
    -- it's 14
    
#2.2
-- use binary features, not real
    -- 0 or 1: keywords appears at least once in a text
-- new forward selection
    -- optimal number is 16
    

#2.3
-- TODO
-- remove correlated features
-- cross-validation


Classification:
-- several common classifiers from sklearn were used
-- baseline is 0.51 (returning always the most frequent class)
-- here are the best results by feature sets:

#1.0 SVM 0.71
#1.1 Decision Tree 0.6777
#2.1 Random Forest 0.7333
#2.2 Linear SVM 0.7778


animal tales vs others, not 7 classes

Whats next?

0) merge our works
finnaly) write a good report, 20 pages



==================

choose
1) improving classification with new features
J
2) try to use the text summaries from web or book
3) look at the new webpage, find more stories
4) clustering/classifying of unknown
J
5) cross validation 

10 June:
merge, find word-net features, have results 

live presentation
